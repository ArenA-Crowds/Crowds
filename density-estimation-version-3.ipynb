{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the variable_kde class (in the dens_estimation module), which is an adapted version from the Scipy \n",
    "kernel density estimation code (scipy/scipy/stats/kde.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy import signal\n",
    "import copy\n",
    "from importlib import reload\n",
    "import dens_estimation as de\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and rebuild data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import the fitted positions data and build a Pandas DataFrame called 'frame'. In 'frame' each row is a fitted position. The columns contain the separate pieces of information accompanying each fit, such as the coordinates, timestamp, and uncertainty values. The rows are ordered by timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# real dataset\n",
    "#F:/ArenaData/arena_fits/2015-07-05.json\n",
    "data = []\n",
    "with open(\"F:/Arena_sim_data/positions_size_20000.json\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "json_lines = []\n",
    "\n",
    "for line in data:\n",
    "    jsline = json.loads(line)\n",
    "    json_lines.append(jsline)\n",
    "\n",
    "frame = pd.DataFrame.from_dict(json_lines)\n",
    "\n",
    "# rebuild dataframe\n",
    "# make dataframe of dicts nested in 'value' column\n",
    "value = pd.DataFrame(list(frame['value']))\n",
    "del frame['value']\n",
    "\n",
    "# make dataframe of dicts nested in 'trackeeHistory' column\n",
    "trackee = pd.DataFrame(list(value['trackeeHistory']))\n",
    "del value['trackeeHistory']\n",
    "\n",
    "#chi2PerDof = pd.DataFrame(list(trackee['chi2PerDof']))\n",
    "#chi2PerDof.columns = ['chi2PerDof']\n",
    "#probChi2 = pd.DataFrame(list(trackee['probChi2']))\n",
    "#probChi2.columns = ['probChi2']\n",
    "#nMeasurements = pd.DataFrame(list(trackee['nMeasurements']))\n",
    "#nMeasurements.columns = ['nMeasurements']\n",
    "localMac = pd.DataFrame(list(trackee['localMac']))\n",
    "localMac.columns = ['localMac']\n",
    "\n",
    "# make dataframe with a 'coordinates' column\n",
    "averagecoordinate = pd.DataFrame(list(value['averagecoordinate']))\n",
    "coordinates = pd.DataFrame(list(averagecoordinate['avg']))\n",
    "averagecoordinate = averagecoordinate.join(coordinates)\n",
    "error = pd.DataFrame(list(averagecoordinate['error']))\n",
    "errorcoordinates = pd.DataFrame(list(error['coordinates']))\n",
    "del errorcoordinates[2]\n",
    "errorcoordinates.columns = ['x_error','y_error']\n",
    "\n",
    "del averagecoordinate['avg']\n",
    "del value['averagecoordinate']\n",
    "\n",
    "# join dataframes\n",
    "frame = frame.join(value.join(averagecoordinate))\n",
    "#frame = frame.join(chi2PerDof)\n",
    "#frame = frame.join(probChi2)\n",
    "frame = frame.join(errorcoordinates)\n",
    "frame = frame.join(localMac)\n",
    "#frame = frame.join(nMeasurements)\n",
    "#del frame['regionsNodesIds']\n",
    "del frame['error']\n",
    "#del frame['type']\n",
    "\n",
    "frame = frame[frame['localMac'] == 0]\n",
    "\n",
    "frame = frame.sort_values(by='measurementTimestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create artificial wi-fi dataframe for e.g. 4 mobile devices\n",
    "# play around with the positions and errors, see what happens\n",
    "\n",
    "data = {'sourceMac': [str(uuid.uuid4()) for i in range(4)],\n",
    "       'measurementTimestamp': [i + 1436047367297 for i in range(1,5)],\n",
    "       'coordinates': [[0,0],[-50,-33],[45,-28],[50,28]],\n",
    "       'x_error': [5,5,5,5],\n",
    "       'y_error': [5,5,5,5]}\n",
    "\n",
    "#frame = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start data analysis code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The density estimation code works according to the following steps:\n",
    "- First a time window is selected from the data\n",
    "- The set of unique MAC addresses in the time window is determined\n",
    "- A bunch of dictionairies are created which hold for each MAC (as key), the required values to construct\n",
    "the density estimate, such as the fitted position coordinates, and the associated uncertainty values\n",
    "- After the first time window, the dictionairies are recreated from the previous set, so the set of MAC addresses\n",
    "can only expand\n",
    "- The density estimates are calculated (using the variable_kde class code)\n",
    "- The density estimates are summed, to create the total crowd density estimate\n",
    "- After each iteration, a deep copy is made of all the density estimates \n",
    "- If a MAC address is not detected in a new time window, the previous density estimate is used (stored in the deep copy)\n",
    "    - This is only done if the history value associated with the MAC address does not exceed the memory parameter\n",
    "    - If the history value does exceed the memory parameter, the density estimate remains zero until the MAC is detected again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here it is assumed that the data is stored in a Pandas DataFrame called 'frame'.\n",
    "The function 'selectWindow' selects the part of the dataset with timestamps falling within the interval specified by variables 'start' and 'stop', and returns a DataFrame with the same structure as 'frame'.\n",
    "Start and stop are specified by the iterator k and the parameters 'interval' and 'timestep'.\n",
    "If timestep > interval, the time windows are non-overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selectWindow(frame, k, init_timestamp, timestep, interval):\n",
    "    #start = min(frame['measurementTimestamp']) + k * timestep\n",
    "    start = init_timestamp + k * timestep\n",
    "    stop = start + interval\n",
    "\n",
    "    window = frame[(frame['measurementTimestamp'] >= start) & \n",
    "                       (frame['measurementTimestamp'] < stop)]\n",
    "\n",
    "    return window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function createDataStructures returns a bunch of dictionairies for the MAC addresses detected in the \n",
    "selected time window, required to do the density estimation later on.\n",
    "It creates a Python dictionairy called 'histos' with the MAC addresses as keys. Each address gets an empty grid (zeros), \n",
    "which is the two-dimensional probability distribution yet to be evaluated.\n",
    "It creates a second dictionairy called 'positions' where each MAC address gets an empty list.\n",
    "It creates two separate dictionairies for the uncertainty values in x and y direction,\n",
    "where each MAC address gets an empty list.\n",
    "It creates a dictionairy called 'history', which for each MAC keeps track of the time that has passed since the last update, given by the number of time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createDataStructures(window, height, width):\n",
    "    grids = np.zeros((len(set(window['sourceMac'])), height,width))\n",
    "\n",
    "    # dictionary of histograms (with mac addresses as keys)\n",
    "    histos = dict(zip(set(window['sourceMac']), grids))\n",
    "    \n",
    "    emptylist = [[] for i in range(len(set(window['sourceMac'])))]\n",
    "    positions = dict(zip(set(window['sourceMac']), emptylist))\n",
    "    emptylist = [[] for i in range(len(set(window['sourceMac'])))]\n",
    "    x_errors = dict(zip(set(window['sourceMac']), emptylist))\n",
    "    emptylist = [[] for i in range(len(set(window['sourceMac'])))]\n",
    "    y_errors = dict(zip(set(window['sourceMac']), emptylist))\n",
    "    \n",
    "    history = dict(zip(set(window['sourceMac']), np.zeros(len(set(window['sourceMac'])))))\n",
    "    \n",
    "    return histos, positions, x_errors, y_errors, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function resetDataStructures all the dictionairies created in createDataStructures are reset: all the MAC addresses get an empty list again.\n",
    "The dictionairy containing the calculated density estimates is deep copied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resetDataStructures(histos, height, width):\n",
    "    \n",
    "    histos_old = copy.deepcopy(histos)\n",
    "    \n",
    "    grids = np.zeros((len(histos), height,width))\n",
    "    histos = dict(zip(histos.keys(), grids))\n",
    "    \n",
    "    emptylist = [[] for i in range(len(histos))]\n",
    "    positions = dict(zip(histos.keys(), emptylist))\n",
    "    emptylist = [[] for i in range(len(histos))]\n",
    "    x_errors = dict(zip(histos.keys(), emptylist))\n",
    "    emptylist = [[] for i in range(len(histos))]\n",
    "    y_errors = dict(zip(histos.keys(), emptylist))\n",
    "    \n",
    "    return histos, histos_old, positions, x_errors, y_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function updateDataStructures all the empty lists in the dictionairies created in createDataStructures are filled with values from the data in the selected time window.\n",
    "If a MAC address is not yet in the dictionary, it is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateDataStructures(window, histos, positions, x_errors, y_errors, history, height, width):\n",
    "    for i in range(len(window)):\n",
    "        if not window['sourceMac'].values[i] in positions:\n",
    "            histos[window['sourceMac'].values[i]] = np.zeros((height,width))\n",
    "            positions[window['sourceMac'].values[i]] = []\n",
    "            x_errors[window['sourceMac'].values[i]] = []\n",
    "            y_errors[window['sourceMac'].values[i]] = []\n",
    "            history[window['sourceMac'].values[i]] = 0\n",
    "            \n",
    "        positions[window['sourceMac'].values[i]].append(window['coordinates'].values[i][:2])\n",
    "        x_errors[window['sourceMac'].values[i]].append(window['x_error'].values[i])\n",
    "        y_errors[window['sourceMac'].values[i]].append(window['y_error'].values[i])\n",
    "        \n",
    "    return histos, positions, x_errors, y_errors, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function createDensityEstimates below uses the variable_kde class from the dens_estimation module.\n",
    "We create an instance of the kernel estimator by passing two (2 x N)-arrays to it, containing the N estimated\n",
    "positions (values), and the N associated uncertainties (errors).\n",
    "The kernel is then evaluated on a provided set of evaluation grid points (gridpoints). The evaluation grid points \n",
    "should be reshaped into a (2 x M)-array (where M is the number of evaluation grid points).\n",
    "The result (the (1 x M)-vector: estimate) is then reshaped back into the size of the evaluation grid.\n",
    "So, for each MAC address we pass a list of estimated positions and a list of two-dimensional errors to the kernel. The kernel returns a list of normalized values for the evaluation grid points.\n",
    "The variable_kde class code is also commented (see dens_estimation.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createDensityEstimates(window, gridpoints, histos, positions, x_errors, y_errors,  height, width):\n",
    "\n",
    "    for mac in histos.keys():\n",
    "        if len(positions[mac]) > 0:\n",
    "            values = np.transpose(np.array(positions[mac]))\n",
    "            uncertainties = np.array([x_errors[mac], y_errors[mac]])\n",
    "            kernel = de.variable_kde(values, uncertainties)\n",
    "            binvals = kernel(gridpoints)\n",
    "            # reshape() stacks row-wise, so we use the Fortran-like index ordering\n",
    "            estimate = np.reshape(binvals, (height,width), order='F')\n",
    "            histos[mac] += estimate\n",
    "            \n",
    "            if histos[mac].sum() > 0:\n",
    "                histos[mac] /= histos[mac].sum()\n",
    "    return histos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing or memorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note: below we see two functions, memorizeNonUpdatedEstimates and smoothNonUpdatedEstimates. If the memory parameter > 0,\n",
    "the smoothNonUpdatedEstimates smoothes non-updated distributions kept in memory. \n",
    "If we decide not to smooth non-updated distributions, we should use the function memorizeNonUpdatedEstimates, \n",
    "which only repeats distributions as they were last detected.\n",
    "We can determine what function is used in the function runDataAnalysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def memorizeNonUpdatedEstimates(histos, histos_old, positions, history, memory):\n",
    "    for mac in histos.keys():\n",
    "        if len(positions[mac]) == 0:\n",
    "            if history[mac] < memory:\n",
    "                histos[mac] += histos_old[mac]\n",
    "                history[mac] += 1\n",
    "            else:\n",
    "                history[mac] = 0\n",
    "    return histos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function smoothNonUpdatedEstimates previously calculated density estimates are smoothed.\n",
    "We first construct a two-dimensional Gaussian bump  of which the width (sigma) is based on by pedestrian walking speed.\n",
    "The two-dimensional function is created using a scipy.stats library function.\n",
    "If there were no detections for a MAC address in the time window, its density estimate from the previous time window \n",
    "(stored in a deep copy) is convoluted with the Gaussian bivariate bump, using a library function from the scipy signal processing module.\n",
    "Each time this is done, the history value associated with the MAC address is incremented.\n",
    "If the history value exceeds the memory parameter value, the density estimate remains zero.\n",
    "The density estimate remains zero, untill the MAC is detected again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_max_velocity(crowd_density):#by Weidmann's equation\n",
    "    rho = crowd_density#avoid division by 0\n",
    "    crowd_density = max(0.1, crowd_density)\n",
    "    v_0=1.34 #speed at 0 density\n",
    "    gamma = 1.913 #fit parameter\n",
    "    rho_max = 5.4 #at this density movement is not possible\n",
    "    v =v_0*(1-np.exp(-gamma*(1/rho-1/rho_max)))\n",
    "    v = max(0.01, v)\n",
    "    #print(\"velocity:\" + str(v))    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smoothNonUpdatedEstimates(histos, histos_old, positions, history, height, width, total_dens_histo):\n",
    "    import math \n",
    "    # generate weighting function with dispersion set to \n",
    "    # Brownian motion with v = 0.5 m/s and t = interval time\n",
    "    # diffusion constant D = (v^2)/2\n",
    "    \n",
    "    #print(\"number of people:\"+str(np.sum(total_dens_histo)))\n",
    "    #print(\"height:\" + str(height))\n",
    "    #print(\"width:\" + str(width))\n",
    "    crowd_density = np.sum(total_dens_histo)/(stadium_length*stadium_width)\n",
    "    #print(\"crowd density:\" + str(crowd_density))\n",
    "    velocity = get_max_velocity(crowd_density)\n",
    "    #D = velocity*velocity/2\n",
    "    t = interval / 1000\n",
    "    #sigma = math.sqrt(2*D*t) / cellsize\n",
    "    sigma = velocity * math.sqrt(t)/cellsize\n",
    "    var = multivariate_normal(mean=[width/2 - 1,height/2 - 1], cov=[[sigma**2,0],[0,sigma**2]])\n",
    "    \n",
    "    weights = np.zeros((height,width))\n",
    "    for i in np.arange(width):\n",
    "        for j in np.arange(height):\n",
    "            weights[j][i] += var.pdf([i,j])\n",
    "    \n",
    "    for mac in histos.keys():\n",
    "        if len(positions[mac]) == 0:\n",
    "            if history[mac] < memory:\n",
    "                # smooth existing pdf from previous time interval\n",
    "                # apply a convolution\n",
    "                conv = signal.convolve2d(histos_old[mac], weights, boundary='wrap', mode='same')\n",
    "                \n",
    "                histos[mac] += conv\n",
    "                history[mac] += 1\n",
    "            else:\n",
    "                history[mac] = 0\n",
    "    \n",
    "    return histos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sumHistograms function simply sums the histograms in the dictionairy 'histos', \n",
    "and returns the total density estimate in the form of a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sumHistograms(histos, height, width):\n",
    "    # total density histogram per period\n",
    "    total_dens_histo = np.zeros((height, width))\n",
    "    \n",
    "    for mac in histos.keys():\n",
    "        total_dens_histo += histos[mac]\n",
    "                \n",
    "    return total_dens_histo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the __main__ function. It runs all the steps, and writes the total density estimate to file.\n",
    "It differentiates between the first and later iterations, in order to initialize the dictionairies,\n",
    "and then only to reset them.\n",
    "Here we also choose whether we smooth non-updated distributions, or just repeat them as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runDataAnalysis(frame, smooth, outputdir, init_timestamp, timestep, height, width, periods, interval, memory,  gridpoints):\n",
    "    total_dens_histo = []\n",
    "    for k in range(periods):\n",
    "        window = selectWindow(frame, k, init_timestamp, timestep, interval)\n",
    "        #if len(window) > 0:\n",
    "        if k < 1:\n",
    "            histos, positions, x_errors, y_errors, history = createDataStructures(window, height, width)\n",
    "            histos, positions, x_errors, y_errors, history =\\\n",
    "            updateDataStructures(window, histos, positions, x_errors, y_errors, history, height, width)\n",
    "            histos = createDensityEstimates(window, gridpoints, histos, positions, x_errors, y_errors, height, width)\n",
    "        else:\n",
    "            histos, histos_old, positions, x_errors, y_errors = resetDataStructures(histos, height, width)\n",
    "            histos, positions, x_errors, y_errors, history =\\\n",
    "            updateDataStructures(window, histos, positions, x_errors, y_errors, history, height, width)\n",
    "            histos = createDensityEstimates(window, gridpoints, histos, positions, x_errors, y_errors, height, width)\n",
    "            if smooth:\n",
    "                histos = smoothNonUpdatedEstimates(histos, histos_old, positions, history, height, width, total_dens_histo)\n",
    "            else:    \n",
    "                histos = memorizeNonUpdatedEstimates(histos, histos_old, positions, history, memory)\n",
    "\n",
    "        total_dens_histo = sumHistograms(histos, height, width)\n",
    "        \n",
    "        np.savetxt(outputdir+'dens_histo_%d.csv' %  k, total_dens_histo, delimiter=',')\n",
    "        #print('Time window:', k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell the parameters are set to run runDataAnalysis. The variable 'cellsize' sets the distance between\n",
    "grid points in the evaluation grid.\n",
    "The variables height and width follow from dividing the size of the evaluation grid (240x180 meter), \n",
    "which is the rectangle containing the Arena stadium, by the cellsize.\n",
    "The variable 'periods' sets the number of time windows to run the analysis.\n",
    "Interval is the length of the time window (in milliseconds). Timestep is the amount of time the time window is moved at each iteration.\n",
    "The memory variable determines the amount of time, measured by the number of time windows,\n",
    "a density estimate is held in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_one():\n",
    "    reload(de)\n",
    "\n",
    "# cell size (bin size)\n",
    "    cellsize = 5;\n",
    "# size of binned region (number of cells)\n",
    "    stadium_width = 105\n",
    "    stadium_length = 70\n",
    "    width = int(stadium_width/cellsize); height = int(stadium_length/cellsize)\n",
    "\n",
    "# build the evalation grid, on which to evaluate the kernel estimator\n",
    "    X, Y = np.mgrid[-int(stadium_width/2):int(stadium_width/2):cellsize,-int(stadium_length/2):int(stadium_length/2):cellsize]\n",
    "    gridpoints = np.vstack([X.ravel(), Y.ravel()])\n",
    "\n",
    "# numbers of time intervals\n",
    "    #periods = 9 #number of time windows\n",
    "    #timestep = 30000 # stride\n",
    "    #interval = 60000 # length of time window in ms\n",
    "    #memory = 9 # in number of time windows\n",
    "    #init_timestamp = 1436047260000\n",
    "\n",
    "    runDataAnalysis(True, \"F:/Arena_sim_data/output/\"  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = \"F:/Arena_sim_data/fake_positions_size_\"\n",
    "crowd_size = 0\n",
    "step = 5000\n",
    "\n",
    "# cell size (bin size)\n",
    "cellsize = 5;\n",
    "# size of binned region (number of cells)\n",
    "stadium_width = 105\n",
    "stadium_length = 70\n",
    "width = int(stadium_width/cellsize); height = int(stadium_length/cellsize)\n",
    "\n",
    "# numbers of time intervals\n",
    "periods = 9 #number of time windows\n",
    "timestep = 30000 # stride\n",
    "interval = 60000 # length of time window in ms\n",
    "memory = 9 # in number of time windows\n",
    "        \n",
    "\n",
    "def run_analysis_for_all(filepath, crowd_size, step, init_timestamp = 1436047260000):\n",
    "    for i in range (0, 16):\n",
    "        crowd_size += step\n",
    "        print(\"crowd_size:\" + str(crowd_size))\n",
    "        data = []\n",
    "        with open(filepath + str(crowd_size) + \".json\") as f:\n",
    "            data = f.readlines()\n",
    "        json_lines = []\n",
    "        for line in data:\n",
    "            jsline = json.loads(line)\n",
    "            json_lines.append(jsline)\n",
    "        global frame    \n",
    "        frame = pd.DataFrame.from_dict(json_lines)\n",
    "# rebuild dataframe\n",
    "# make dataframe of dicts nested in 'value' column\n",
    "        value = pd.DataFrame(list(frame['value']))\n",
    "        del frame['value']\n",
    "# make dataframe of dicts nested in 'trackeeHistory' column\n",
    "        trackee = pd.DataFrame(list(value['trackeeHistory']))\n",
    "        del value['trackeeHistory']\n",
    "        localMac = pd.DataFrame(list(trackee['localMac']))\n",
    "        localMac.columns = ['localMac']\n",
    "# make dataframe with a 'coordinates' column\n",
    "        averagecoordinate = pd.DataFrame(list(value['averagecoordinate']))\n",
    "        coordinates = pd.DataFrame(list(averagecoordinate['avg']))\n",
    "        averagecoordinate = averagecoordinate.join(coordinates)\n",
    "        error = pd.DataFrame(list(averagecoordinate['error']))\n",
    "        errorcoordinates = pd.DataFrame(list(error['coordinates']))\n",
    "        del errorcoordinates[2]\n",
    "        errorcoordinates.columns = ['x_error','y_error']\n",
    "        del averagecoordinate['avg']\n",
    "        del value['averagecoordinate']\n",
    "# join dataframes\n",
    "        frame = frame.join(value.join(averagecoordinate))\n",
    "        frame = frame.join(errorcoordinates)\n",
    "        frame = frame.join(localMac)\n",
    "        del frame['error']\n",
    "        frame = frame[frame['localMac'] == 0]\n",
    "        frame = frame.sort_values(by='measurementTimestamp')\n",
    "        reload(de)        \n",
    "        # build the evalation grid, on which to evaluate the kernel estimator\n",
    "        X, Y = np.mgrid[-int(stadium_width/2):int(stadium_width/2):cellsize,-int(stadium_length/2):int(stadium_length/2):cellsize]\n",
    "        gridpoints = np.vstack([X.ravel(), Y.ravel()])\n",
    "        runDataAnalysis(frame, False,\"F:/Arena_sim_data/output/size_\"+ str(crowd_size) + \"_\", 1436047260000, timestep,height, width, periods, interval, memory,  gridpoints)\n",
    "        #runDataAnalysis(frame, smooth, outputdir, init_timestamp= 1436047260000, timestep= 30000, height, width, periods, interval, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crowd_size:5000\n",
      "crowd_size:10000\n",
      "crowd_size:15000\n",
      "crowd_size:20000\n",
      "crowd_size:25000\n",
      "crowd_size:30000\n",
      "crowd_size:35000\n",
      "crowd_size:40000\n",
      "crowd_size:45000\n",
      "crowd_size:50000\n",
      "crowd_size:55000\n",
      "crowd_size:60000\n",
      "crowd_size:65000\n",
      "crowd_size:70000\n",
      "crowd_size:75000\n",
      "crowd_size:80000\n"
     ]
    }
   ],
   "source": [
    "run_analysis_for_all(filepath, crowd_size, step, 1436047260000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function runDataAnalysis writes results to file. Below we read in those files again in order to plot them. \n",
    "Make sure the path and file names are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get the maximum occuring density value to set the size of the z-axis in the 3-D plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check maximum value for z-axis limit\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "maxValue = 0\n",
    "\n",
    "for i in range(periods):\n",
    "    temp = np.loadtxt('F:/Arena_sim_data/output/dens_histo_%d.csv' % i, delimiter=',').max()\n",
    "    if temp > maxValue:\n",
    "        maxValue = temp\n",
    "        \n",
    "#maxValue = ceil(maxValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,10))\n",
    "\n",
    "#col = ['r', 'y', 'c', 'k', 'c','r'] * height * width\n",
    "col = ['w','r','w','w','w','w'] * height * width\n",
    "# colors = np.random.choice(col, height*width)\n",
    "\n",
    "for k in range(periods):\n",
    "    \n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    x_data, y_data = np.meshgrid( np.arange(width),\n",
    "                                  np.arange(height)*(-1) )\n",
    "\n",
    "    x_data = x_data.flatten()\n",
    "    y_data = y_data.flatten()\n",
    "\n",
    "    z_data = np.loadtxt('F:/Arena_sim_data/output/dens_histo_%s.csv' % k, delimiter=',').flatten()\n",
    "    #z_data = total_dens_histos[k].flatten()\n",
    "    ax.set_zlim3d(0, maxValue)\n",
    "    ax.bar3d( x_data,\n",
    "              y_data,\n",
    "              np.zeros(len(z_data)),\n",
    "              1, 1, z_data, color=col) # \n",
    "    if k < 10:\n",
    "        number = '000' + str(k)\n",
    "    elif k > 9:\n",
    "        number = '00' + str(k)\n",
    "    elif k > 99:\n",
    "        number = '0' + str(k)\n",
    "    plt.savefig('F:/Arena_sim_data/output/dens_histo_%s.png' % number)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_data[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
